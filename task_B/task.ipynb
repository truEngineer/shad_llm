{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AtspjpqElmtO"
   },
   "source": [
    "# Inference Speedup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wKxd_dT3lmtQ"
   },
   "source": [
    "## Подготовка\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "axGKC6hXlmtR",
    "outputId": "553dca24-fd13-411a-a322-e7cb48733b10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.5.1+cu124\n",
      "triton version: 3.1.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from typing import List\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "print(\"torch version:\", torch.__version__)\n",
    "print(\"triton version:\", triton.__version__)\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X3NJfn9nlmtS"
   },
   "source": [
    "### Catch activation ans weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0,
     "referenced_widgets": [
      "61621d16e2cd40c3ad0c6a849a8b8264",
      "6bed81d4616649bd857418a131538f7d",
      "9ffb3a0c0fea4081bf4f7c5e6218758f",
      "3838c3cdee884f6d902e3b5d0d4e4583",
      "7c3c03fa91874ae5ac941ed171eeb3ea",
      "7498bbbbae7048119584dea4a914da0a",
      "6272889cfea04dab834ee1c62817540b",
      "5f922ecfa23c4705a42336c1e79900df",
      "7f1074b6abde4dddb7e50f67dc9a99b3",
      "87caf0f9f3dc4b508d7733e66fde5407",
      "c0e17b91d9b244daaf63f87f4683503e"
     ]
    },
    "id": "UI2qK8TdlmtT",
    "outputId": "23e582d5-f21a-4fd4-b1d9-13470ba365e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61621d16e2cd40c3ad0c6a849a8b8264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "streamer = TextStreamer(tokenizer, skip_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5GbVOeSXlmtT"
   },
   "outputs": [],
   "source": [
    "# challenging input\n",
    "prompt = \"\"\"\n",
    "# SYSTEM PREAMBLE\n",
    "1) You are an excellent Python software developer with over 10 years of experience. You have a strong understanding of Python related topics, data structures, libraries, frameworks, algorithms, best practices and optimization techniques.\n",
    "2) You are here to help the user (the software developer) by breaking his request in ## TASK into logical steps and writing high-quality and efficient code to implement each step.\n",
    "3) You have to return the entire code.\n",
    "4) Follow \"Answering rules\" without exception.\n",
    "\n",
    "## ANSWERING RULES\n",
    "1) Repeat the question before answering it.\n",
    "2) Always follow \"CHAIN OF THOUGHTS\" to execute the task.\n",
    "\n",
    "## CHAIN OF THOUGHTS\n",
    "1) **OBEY the EXECUTION MODE**\n",
    "2) **TASK ANALYSIS:**\n",
    "   - Understand the user's request thoroughly.\n",
    "   - Identify the key components and requirements of the task.\n",
    "3) **PLANNING: CODDING:**\n",
    "   - Break down the task into logical, sequential steps.\n",
    "   - Outline the strategy for implementing each step.\n",
    "4) **CODING:**\n",
    "   - Explain your thought process before writing any code.\n",
    "   - Write the entire code for each step, ensuring it is clean, optimized, and well-commented.\n",
    "   - Handle edge cases and errors appropriately.\n",
    "5) **VERIFICATION:**\n",
    "   - Review the complete code solution for accuracy and efficiency.\n",
    "   - Ensure the code meets all requirements and is free of errors.\n",
    "\n",
    "## TASK\n",
    "\n",
    "Write a python function that receives the following JSON as input and enters data from it into the Google Sheet.\n",
    "\n",
    "{\n",
    "    'date': '31-05-2024',\n",
    "    'revenue': 90000,\n",
    "    'person' : 'User1',\n",
    "    'expensesList': [30000, 14000, 10000, 2000, 15000],\n",
    "    'expensesDescList': [ 'Ключи', 'Ключи2', 'Счет за такси', 'Клей, пластины', 'Провод 40м'],\n",
    "    'expensesTypeList': ['Закупки', 'Закупки', 'Расходы', 'Ремонт', 'Ремонт']\n",
    "}\n",
    "\n",
    "There is a date in JSON, you can use it to determine the month.\n",
    "The data is entered into a list with the name of the month. If such a list does not exist yet, then you need to create a list with a new month inside the sheet.\n",
    "\n",
    "The list should have the following columns (the first rows are used as headings):\n",
    "A1: Дата расхода,\n",
    "B1: сумма расхода,\n",
    "C1: описание расхода,\n",
    "D1: тип расхода,\n",
    "E1: кто внес данные\n",
    "\n",
    "G1: Дата выручки\n",
    "H1: Сумма выручки\n",
    "I1: Кто внес данные\n",
    "\n",
    "Please separate expenses and profits with a blank column.\n",
    "Please sort expenses by date, including those already listed in Google sheet list.\n",
    "Please sort earnings by date, including those already listed in Google sheet list.\n",
    "\n",
    "It is prohibited to use oauth2client as it is deprecated.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "scDTImShlmtU",
    "outputId": "550cdf9f-6118-46c1-83ea-ab4b3c1059ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To accomplish the task, we will follow these steps:\n",
      "\n",
      "1. Parse the JSON data.\n",
      "2. Create or update the expense and profit lists in the Google Sheet.\n",
      "3. Sort both lists by date.\n",
      "4. Insert the data into the appropriate columns in the Google Sheet.\n",
      "\n",
      "Let's start by parsing the JSON data and creating the necessary lists. For simplicity, we will assume that the Google Sheet has been set up correctly and that we have access to the necessary Google Sheets API client library. We'll use `google-auth` and `google-api-python-client` for authentication and interaction with the Google Sheets API.\n",
      "\n",
      "Here's the complete code:\n",
      "\n",
      "```python\n",
      "import json\n",
      "from datetime import datetime\n",
      "from google.oauth2.credentials import Credentials\n",
      "from googleapiclient.discovery import build\n",
      "\n",
      "def parse_json(json_data):\n",
      "    \"\"\"Parse the JSON data.\"\"\"\n",
      "    parsed_data = json.loads(json_data)\n",
      "    \n",
      "    # Extract relevant information\n",
      "    date_str = parsed_data['date']\n",
      "    revenue = parsed_data['revenue']\n",
      "    person = parsed_data['person']\n",
      "    expenses = parsed_data['expensesList']\n",
      "    expenses_desc = parsed_data['expensesDescList']\n",
      "    expenses_type = parsed_data['expensesTypeList']\n",
      "    \n",
      "    return {\n",
      "        'date': date_str,\n",
      "        'revenue': revenue,\n",
      "        'person': person,\n",
      "        'expenses': expenses,\n",
      "        'expenses_desc': expenses_desc,\n",
      "        'expenses_type': expenses_type\n",
      "    }\n",
      "\n",
      "def get_sheet_service(credentials):\n",
      "    \"\"\"Get the Google Sheets service.\"\"\"\n",
      "    service = build('sheets', 'v4', credentials=credentials)\n",
      "    return service.spreadsheets()\n",
      "\n",
      "def add_row_to_sheet(sheet_id, service, row, sheet_name='Sheet1'):\n",
      "    \"\"\"Add a row to the specified sheet.\"\"\"\n",
      "    body = {\n",
      "        'values': [row]\n",
      "    }\n",
      "    result = service.spreadsheets().values().append(\n",
      "        spreadsheetId=sheet_id,\n",
      "        valueInputOption='USER_ENTERED',\n",
      "        range=f'{sheet_name}!A1:E'\n",
      "    ).execute()\n",
      "    print(f\"{result.get('updates').get('updatedCells')} cells updated.\")\n",
      "    return result\n",
      "\n",
      "def add_profit_row_to_sheet(sheet_id, service, row, sheet_name='Sheet1'):\n",
      "    \"\"\"Add a row for profit to the specified sheet.\"\"\"\n",
      "    body = {\n",
      "        'values': [row]\n",
      "    }\n",
      "    result = service.spreadsheets().values().append(\n",
      "        spreadsheetId=sheet_id,\n",
      "        valueInputOption='USER_ENTERED',\n",
      "        range=f'{sheet_name}!G1:I'\n",
      "    ).execute()\n",
      "    print(f\"{result.get('updates').get('updatedCells')} cells updated.\")\n",
      "    return result\n",
      "\n",
      "def get_date_month(date_str):\n",
      "    \"\"\"Get the month from the date string.\"\"\"\n",
      "    date_obj = datetime.strptime(date_str, '%d-%m-%Y')\n",
      "    return date_obj.strftime('%B')\n",
      "\n",
      "def main():\n",
      "    # Load credentials from JSON file\n",
      "    with open('path/to/your/credentials.json') as f:\n",
      "        credentials = Credentials.from_authorized_user_file(f)\n",
      "    \n",
      "    # Get the Google Sheets service\n",
      "    service = get_sheet_service(credentials)\n",
      "    \n",
      "    # Parse the JSON data\n",
      "    parsed_data = parse_json(json_data)\n",
      "    \n",
      "    # Determine the month\n",
      "    month = get_date_month(parsed_data['date'])\n",
      "    \n",
      "    # Prepare expense row\n",
      "    expenses_row = [\n",
      "        parsed_data['date'],\n",
      "        str(parsed_data['revenue']),\n",
      "        ', '.join(parsed_data['expenses_desc']),\n",
      "        ', '.join(parsed_data['expenses_type']),\n",
      "        parsed_data['person']\n",
      "    ]\n",
      "    \n",
      "    # Prepare profit row\n",
      "    profit_row = [\n",
      "        parsed_data['date'],\n",
      "        str(parsed_data['revenue']),\n",
      "        'Выручка',\n",
      "        'Себестоимость',\n",
      "        parsed_data['person']\n",
      "    ]\n",
      "    \n",
      "    # Add expense row to the sheet\n",
      "    add_row_to_sheet(service.spreadsheet_id, service, expenses_row, sheet_name='Sheet1')\n",
      "    \n",
      "    # Add profit row to the sheet\n",
      "    add_profit_row_to_sheet(service.spreadsheet_id, service, profit_row, sheet_name='Sheet1')\n",
      "\n",
      "if __name__ == \"__main__\":\n",
      "    json_data = '''\n",
      "    {\n",
      "        'date': '31-05-2024',\n",
      "        'revenue': 90000,\n",
      "        'person': 'User1',\n",
      "        'expensesList': [30000, 14000, 10000, 2000, 15000],\n",
      "        'expensesDescList': [ 'Ключи', 'Ключи2', 'Счет за такси', 'Клей, пластины', 'Провод 40м'],\n",
      "        'expensesTypeList': ['Закупки', 'Закупки', 'Расходы', 'Ремонт', 'Ремонт']\n",
      "    }\n",
      "    '''\n",
      "    main()\n",
      "```\n",
      "\n",
      "### Explanation:\n",
      "\n",
      "1. **Parsing JSON\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "generation_output = model.generate(**model_inputs, streamer=streamer, max_new_tokens=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "GmZ09xWelmtV"
   },
   "outputs": [],
   "source": [
    "class Catcher(nn.Module):\n",
    "    def __init__(self, inps: List, module: nn.Module):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.inps = inps\n",
    "\n",
    "    def forward(self, inp, **kwargs):\n",
    "        self.inps.append(inp.to(\"cpu\"))\n",
    "        raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UaQPJeQMlmtV"
   },
   "outputs": [],
   "source": [
    "layer = model.model.layers[0]\n",
    "inps = []\n",
    "layer.self_attn.q_proj = Catcher(inps, layer.self_attn.q_proj) # wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "lDXTyhi4lmtV"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    model(model_inputs.input_ids)\n",
    "except ValueError as e:\n",
    "    layer.self_attn.q_proj = layer.self_attn.q_proj.module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N4wTaZR5lmtV",
    "outputId": "9f494935-5a56-49fa-db61-2042d56ba9e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 700, 2048])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inps[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "9p1tFHE5lmtV"
   },
   "outputs": [],
   "source": [
    "weight = layer.self_attn.q_proj.weight # unwrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mH2g_HxalmtV"
   },
   "outputs": [],
   "source": [
    "def is_cuda():\n",
    "    return triton.runtime.driver.active.get_current_target().backend == \"cuda\"\n",
    "\n",
    "assert is_cuda(), \"CUDA only tutorial\"\n",
    "ref_lib = 'cuBLAS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "I1Il52RRlmtW"
   },
   "outputs": [],
   "source": [
    "def get_cuda_autotune_config():\n",
    "    return [\n",
    "        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n",
    "        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,  'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,  'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64,  'BLOCK_SIZE_N': 32,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n",
    "        triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32,  'BLOCK_SIZE_N': 64,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n",
    "\t    triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,  'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "\t    triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,  'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n",
    "\t    triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n",
    "\t    triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,  'GROUP_SIZE_M': 16}, num_stages=4, num_warps=4),\n",
    "\t    triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,  'GROUP_SIZE_M': 16}, num_stages=3, num_warps=8),\n",
    "\t    triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n",
    "\t    triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,  'GROUP_SIZE_M': 16}, num_stages=4, num_warps=4),\n",
    "\t    triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16}, num_stages=3, num_warps=8),\n",
    "\t    triton.Config({'SPLIT_K': 1, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n",
    "        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n",
    "        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64,  'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64,  'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64,  'BLOCK_SIZE_N': 32,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n",
    "        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32,  'BLOCK_SIZE_N': 64,  'BLOCK_SIZE_K': 32, 'GROUP_SIZE_M': 8}, num_stages=5, num_warps=2),\n",
    "        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,  'GROUP_SIZE_M': 8}, num_stages=4, num_warps=4),\n",
    "\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,  'GROUP_SIZE_M': 8}, num_stages=3, num_warps=8),\n",
    "        triton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 8}, num_stages=2, num_warps=4),\n",
    "\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32,  'GROUP_SIZE_M': 16}, num_stages=4, num_warps=4),\n",
    "\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,  'GROUP_SIZE_M': 16}, num_stages=3, num_warps=8),\n",
    "\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n",
    "\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 64,  'GROUP_SIZE_M': 16}, num_stages=4, num_warps=4),\n",
    "\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 128, 'GROUP_SIZE_M': 16}, num_stages=3, num_warps=8),\n",
    "\t\ttriton.Config({'SPLIT_K': 2, 'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 256, 'GROUP_SIZE_M': 16}, num_stages=2, num_warps=4),\n",
    "\n",
    "        # custom\n",
    "        # !!! TASK: add some tilings for decode scenario\n",
    "        # hint: BLOCK_SIZE_M should be small\n",
    "    ]\n",
    "\n",
    "\n",
    "def get_autotune_config():\n",
    "    if is_cuda():\n",
    "        return get_cuda_autotune_config()\n",
    "    raise NotImplementedError(\"ooops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lZEDKGdlmtW"
   },
   "source": [
    "## Задача"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QXEdxgI0lmtW"
   },
   "source": [
    "### Dynamic W8A8 GeMM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Важный дисклеймер:** пожалуйста, выполняйте задачу к google colab на среде T4, потому что на ней эта задача оттестирована и уже установлены правильные зависимости по умолчанию, это сэкономит вам кучу времени. \n",
    "\n",
    "Для непослушных: на другом типе видеокарт (H100) код придется сильно переписывать, чтобы учесть архитектурные особенности для максимальной производительности (см. \"менее полезное, но интересное\" выше в семинаре).\n",
    "\n",
    "> **Важный дисклеймер 2:** для выполнения задачи нужно запустить код выше, в нем есть нужные helper функции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1WLBIKEelmtX"
   },
   "source": [
    "Useful resources:\n",
    "- [Matrix Multiplication Background User's Guide](https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html)\n",
    "- [Deep Dive on CUTLASS Ping-Pong GEMM Kernel](https://pytorch.org/blog/cutlass-ping-pong-gemm-kernel/)\n",
    "- [Accelerating 2D Dynamic Block Quantized Float8 GEMMs in Triton](https://pytorch.org/blog/accelerating-gemms-triton/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://habrastorage.org/webt/xq/r5/8a/xqr58aw0gd6tdm-j45yqad67w1a.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании вам нужно будет реализовать быструю операцию квантизации в per-row режиме:\n",
    "1. Для этого нужно опять же заполнить пропуски возле `YOUR CODE HERE`, пожалуйста не удаляйте эти комментарии с заданием и обозначением, это облегчает проверку\n",
    "2. Рядом с каждым пропуском есть комментарий с `# !!! TASK: ...`, который поможет разобраться что именно нужно написать\n",
    "3. На иллюстрации выше как раз показано по каким именно размерностям для матриц активаций и весов считаются scales\n",
    "4. Формула для `scales = tensor.abs().max(axis=axis) / INT8_max_value`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "1_BCSvnblmtX"
   },
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    configs=[\n",
    "        triton.Config({}, num_stages=2, num_warps=8),\n",
    "        triton.Config({}, num_stages=2, num_warps=4),\n",
    "        triton.Config({}, num_stages=2, num_warps=2),\n",
    "        triton.Config({}, num_stages=2, num_warps=1),\n",
    "     ],\n",
    "    key=['K'],\n",
    ")\n",
    "@triton.jit\n",
    "def quantize_int8_perrow_kernel(\n",
    "    fpa_ptr, a_ptr, as_ptr,\n",
    "    M, K,\n",
    "    stride_fpam, stride_fpak,\n",
    "    stride_am, stride_ak,\n",
    "    stride_asm,\n",
    "    # Meta-parameters\n",
    "    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n",
    "):\n",
    "    pid_m = tl.program_id(axis=0)\n",
    "    offs_k = tl.arange(0, BLOCK_SIZE_K)\n",
    "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "\n",
    "    fpa_ptrs = fpa_ptr + offs_am[:, None] * stride_fpam + offs_k[None, :] * stride_fpak\n",
    "    a_ptrs = a_ptr + offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak\n",
    "    a_max = tl.zeros((BLOCK_SIZE_M,), dtype=tl.float32)\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        fpa = tl.load(fpa_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "\n",
    "        # !!! TASK: calc maximum absolute value of each row of fpa and update a_max\n",
    "        # YOUR CODE HERE\n",
    "        a_max = \n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        fpa_ptrs += BLOCK_SIZE_K * stride_fpak\n",
    "\n",
    "    # !!! TASK: divide a_max by max positive INT8 value\n",
    "    # YOUR CODE HERE\n",
    "    a_scale = \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    fpa_ptrs = fpa_ptr + offs_am[:, None] * stride_fpam + offs_k[None, :] * stride_fpak\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):\n",
    "        fpa = tl.load(fpa_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)\n",
    "\n",
    "        # !!! TASK: divide fpa by a_scale and convert to INT8\n",
    "        # YOUR CODE HERE\n",
    "        inta = \n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        tl.store(a_ptrs, inta, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K)\n",
    "        fpa_ptrs += BLOCK_SIZE_K * stride_fpak\n",
    "        a_ptrs += BLOCK_SIZE_K * stride_ak\n",
    "    as_offs = pid_m * BLOCK_SIZE_M * stride_asm + tl.arange(0, BLOCK_SIZE_M)\n",
    "    tl.store(as_ptr + as_offs, a_scale)\n",
    "\n",
    "\n",
    "def quantize_int8_perrow(fpa):\n",
    "    a = torch.empty(fpa.shape, device=fpa.device, dtype=torch.int8)\n",
    "    a_scale = torch.empty(fpa.shape[0], device=fpa.device, dtype=fpa.dtype)\n",
    "    M, K = fpa.shape\n",
    "    BLOCK_SIZE_M = 1\n",
    "    BLOCK_SIZE_K = triton.next_power_of_2(K)\n",
    "    grid = (M // BLOCK_SIZE_M,)\n",
    "    quantize_int8_perrow_kernel[grid](\n",
    "        fpa, a, a_scale,\n",
    "        M, K,\n",
    "        fpa.stride(0), fpa.stride(1),\n",
    "        a.stride(0), a.stride(1),\n",
    "        a_scale.stride(0),\n",
    "        BLOCK_SIZE_M, BLOCK_SIZE_K,\n",
    "    )\n",
    "    return a, a_scale\n",
    "\n",
    "\n",
    "def quantize_int8(weight, axis=0, tp_rank=0):\n",
    "    # Weight shape: [H1, H2]\n",
    "    # Scale shape: [H2]\n",
    "    \n",
    "    # !!! TASK: calculate scale by taking maximum over axis and saving dims and divide on maximum positive INT8 value\n",
    "    # YOUR CODE HERE\n",
    "    scale = \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # !!! TASK: divide weight by scale and convert to int8\n",
    "    # YOUR CODE HERE\n",
    "    weight = \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # col major will accelerate i8xi8 kernel.\n",
    "    if axis == 0:\n",
    "        weight = weight.t().contiguous().t()\n",
    "    scale = scale.squeeze(axis)\n",
    "    return weight.contiguous().cuda(tp_rank), scale.contiguous().cuda(tp_rank)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее нам предстоит реализовать уже быструю операцию для per-row W8A8 GeMM:\n",
    "1. Для этого нужно опять же заполнить пропуски возле `YOUR CODE HERE`, пожалуйста не удаляйте эти комментарии с заданием и обозначением, это облегчает проверку\n",
    "2. Рядом с каждым пропуском есть комментарий с `# !!! TASK: ...`, который поможет разобраться что именно нужно написать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Qee3RRE4lmtX"
   },
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    configs=get_autotune_config(),\n",
    "    key=['M', 'N', 'K'],\n",
    "    reset_to_zero=['c_ptr']\n",
    ")\n",
    "@triton.jit\n",
    "def perrow_w8a8_matmul_kernel(\n",
    "    # Pointers to matrices\n",
    "    a_ptr, as_ptr, b_ptr, bs_ptr, c_ptr,\n",
    "    # Matrix dimensions\n",
    "    M, N, K,\n",
    "    # The stride variables represent how much to increase the ptr by when moving by 1\n",
    "    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n",
    "    # by to get the element one row down (A has M rows).\n",
    "    stride_am, stride_ak,\n",
    "    stride_asm,\n",
    "    stride_bk, stride_bn,\n",
    "    stride_bsn,\n",
    "    stride_cm, stride_cn,\n",
    "    # Meta-parameters\n",
    "    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n",
    "    GROUP_SIZE_M: tl.constexpr, SPLIT_K: tl.constexpr,\n",
    "):\n",
    "    \"\"\"Kernel for computing the matmul C = A x B.\n",
    "    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n",
    "    \"\"\"\n",
    "    # -----------------------------------------------------------\n",
    "    # Map program ids `pid` to the block of C it should compute.\n",
    "    # This is done in a grouped ordering to promote L2 data reuse.\n",
    "    # See above `L2 Cache Optimizations` section for details.\n",
    "    pid = tl.program_id(axis=0)\n",
    "    pid_sp_k = tl.program_id(axis=1)\n",
    "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
    "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_m = group_id * GROUP_SIZE_M\n",
    "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "    pid_m = first_pid_m + (pid % group_size_m)\n",
    "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Create pointers for the first blocks of A and B.\n",
    "    # We will advance this pointer as we move in the K direction\n",
    "    # and accumulate\n",
    "    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n",
    "    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n",
    "    # See above `Pointer Arithmetics` section for details\n",
    "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n",
    "    offs_k = pid_sp_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "    as_ptrs = as_ptr + offs_am * stride_asm\n",
    "    bs_ptrs = bs_ptr + offs_bn * stride_bsn\n",
    "    a_scale = tl.load(as_ptrs, mask=offs_am < M, other=0.0)\n",
    "    b_scale = tl.load(bs_ptrs, mask=offs_bn < N, other=0.0)\n",
    "    # -----------------------------------------------------------\n",
    "    # Iterate to compute a block of the C matrix.\n",
    "    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n",
    "    # of fp32 values for higher accuracy.\n",
    "    # `accumulator` will be converted back to fp16 after the loop.\n",
    "    \n",
    "    # !!! TASK: create accumulator of int32 dtype\n",
    "    # YOUR CODE HERE\n",
    "    accumulator = \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):\n",
    "        # Load the next block of A and B, generate a mask by checking the K dimension.\n",
    "        # If it is out of bounds, set it to 0.\n",
    "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K * SPLIT_K, other=0.0)\n",
    "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K * SPLIT_K, other=0.0)\n",
    "        # We accumulate along the K dimension.\n",
    "\n",
    "        # !!! TASK: update accumulator with a @ b\n",
    "        # YOUR CODE HERE\n",
    "        accumulator += \n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        # Advance the ptrs to the next K block.\n",
    "        a_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_ak\n",
    "        b_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_bk\n",
    "    # You can fuse arbitrary activation functions here\n",
    "    # while the accumulator is still in FP32!\n",
    "    \n",
    "    # !!! TASK: dequantize the accumulator with a_scale and b_scale (outer product) and convert to c_ptr.dtype.element_ty\n",
    "    # YOUR CODE HERE\n",
    "    c = \n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    # -----------------------------------------------------------\n",
    "    # Write back the block of the output matrix C with masks.\n",
    "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "    if SPLIT_K == 1:\n",
    "        tl.store(c_ptrs, c, mask=c_mask)\n",
    "    else:\n",
    "        tl.atomic_add(c_ptrs, c, mask=c_mask)\n",
    "\n",
    "\n",
    "def matmul_int8(a, a_scale, b, b_scale, out=None):\n",
    "    # Check constraints.\n",
    "    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n",
    "    M, K = a.shape\n",
    "    K, N = b.shape\n",
    "    # Allocates output.\n",
    "    if out == None:\n",
    "        c = torch.zeros((M, N), device=a.device, dtype=torch.float16)\n",
    "    else:\n",
    "        c = out.fill_(0.)\n",
    "    grid = lambda META: (\n",
    "        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n",
    "        META['SPLIT_K'],\n",
    "    )\n",
    "    perrow_w8a8_matmul_kernel[grid](\n",
    "        a, a_scale, b, b_scale, c,\n",
    "        M, N, K,\n",
    "        a.stride(0), a.stride(1),\n",
    "        a_scale.stride(0),\n",
    "        b.stride(0), b.stride(1),\n",
    "        b_scale.stride(0),\n",
    "        c.stride(0), c.stride(1),\n",
    "    )\n",
    "    return c\n",
    "\n",
    "\n",
    "def matmul_quantize_int8(fpa, b, b_scale, out=None):\n",
    "    # !!! TASK: quantize fpa to int8 and call matmul_int8\n",
    "    # YOUR CODE HERE\n",
    "    a, a_scale = \n",
    "    return \n",
    "    # YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtcILz_IlmtX"
   },
   "source": [
    "Sanity check (heavily relaxed for non-smoothed case):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mvuppE_4lmtX",
    "outputId": "56cb7ab9-9bf1-4c6d-c354-a12fd76d2606"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Triton FP8 and Torch match\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "cos = torch.nn.CosineSimilarity(0)\n",
    "\n",
    "a = inps[0][0].to(torch.float16).cuda()\n",
    "b = weight.to(torch.float16).cuda()\n",
    "\n",
    "int_a, scale_a = quantize_int8_perrow(a)\n",
    "int_b, scale_b = quantize_int8(b, axis=0)\n",
    "\n",
    "\n",
    "triton_output = matmul_int8(int_a, scale_a, int_b, scale_b)\n",
    "torch_output = torch.matmul(a, b)\n",
    "\n",
    "if (torch_output.float() - triton_output.float()).abs().mean() < 0.03 and torch.quantile((torch_output.float() - triton_output.float()).abs(), 0.95) < 0.07:\n",
    "    print(\"✅ Triton FP8 and Torch match\")\n",
    "else:\n",
    "    print(\"❌ Triton FP8 and Torch differ\")\n",
    "    print(\"Quantization cos: \", cos((int_a * scale_a.unsqueeze(1)).flatten().to(torch.float32), a.flatten().to(torch.float32)).item())\n",
    "\n",
    "    print('=' * 50)\n",
    "    print(f\"triton_output_with_fp8={triton_output}\")\n",
    "    print(f\"torch_output={torch_output}\")\n",
    "\n",
    "    print('=' * 50)\n",
    "    print(\"infs in triton:\", (triton_output).isinf().sum())\n",
    "    print(\"infs in torch:\", (torch_output).isinf().sum())\n",
    "\n",
    "    print('=' * 50)\n",
    "    print(\"Output cos:\", cos(triton_output.flatten().to(torch.float32), torch_output.flatten().to(torch.float32)).item())\n",
    "    print(((triton_output - torch_output).abs() >= 0.5).sum())\n",
    "    print(((triton_output - torch_output).abs() / (torch_output.abs() + 1e-5)))\n",
    "    assert False, \"Triton and Torch differ\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBItEEG0lmtX"
   },
   "source": [
    "Sanity checks on perf:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "EDGVPDjRlmtX"
   },
   "outputs": [],
   "source": [
    "def test_perf_quantize(M: int, K: int, iters: int = 256, thr: float = 1.3):\n",
    "    torch.manual_seed(0)\n",
    "    print(f\"M: {M} K: {K}\")\n",
    "\n",
    "    a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n",
    "    # warmup\n",
    "    for _ in range(10):\n",
    "        int_a, a_scale = quantize_int8(a, 1)\n",
    "        int_a, a_scale = quantize_int8_perrow(a)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    t1 = time.time()\n",
    "    for _ in range(iters):\n",
    "        int_a, a_scale = quantize_int8_perrow(a)\n",
    "    torch.cuda.synchronize()\n",
    "    t2 = time.time()\n",
    "    for _ in range(iters):\n",
    "        int_a, a_scale = quantize_int8(a, axis=1)\n",
    "    torch.cuda.synchronize()\n",
    "    t3 = time.time()\n",
    "\n",
    "    torch_time = (t3 - t2) / iters\n",
    "    triton_time = (t2 - t1) / iters\n",
    "\n",
    "    print(f\"Torch time cost: {torch_time}\")\n",
    "    print(f\"Triton time cost: {triton_time}\")\n",
    "    assert torch_time / triton_time > thr, f\"Must get at least {thr}x speedup\"\n",
    "    return triton_time, torch_time\n",
    "\n",
    "\n",
    "def test_perf_matmul_int8(M, K, N, iters: int = 512, thr: float = 0.99):\n",
    "    print(\"M: {} K: {} N: {}\".format(M, K, N))\n",
    "    torch.manual_seed(0)\n",
    "    a = torch.randn((M, K), device='cuda', dtype=torch.float16)\n",
    "    b = torch.randn((K, N), device='cuda', dtype=torch.float16).contiguous()\n",
    "    int_b, scale_b = quantize_int8(b, axis=0)\n",
    "    for _ in range(10):\n",
    "        # int_a, a_scale = quantize_int8(a, 1)\n",
    "        int_a, a_scale = quantize_int8_perrow(a)\n",
    "        triton_output = matmul_int8(int_a, a_scale, int_b, scale_b)\n",
    "    torch.cuda.synchronize()\n",
    "    t1 = time.time()\n",
    "    for _ in range(iters):\n",
    "        #int_a, a_scale, _ = quantize_int8(a, 1)\n",
    "        int_a, a_scale = quantize_int8_perrow(a)\n",
    "    torch.cuda.synchronize()\n",
    "    qt2 = time.time()\n",
    "    for _ in range(iters):\n",
    "        triton_output = matmul_int8(int_a, a_scale, int_b, scale_b)\n",
    "    torch.cuda.synchronize()\n",
    "    t2 = time.time()\n",
    "    quant_time = qt2 - t1\n",
    "    triton_time = t2 - qt2\n",
    "    triton_tflops = 2 * M * N * K * 1e-12 / (triton_time / iters)\n",
    "    quant_bandwith = 2 * M * K * 1e-9 / (quant_time / iters)\n",
    "    print(\"Triton time cost: {} (tflops {}) + quant: {} (bandwidth {})\".format(\n",
    "        triton_time, triton_tflops, quant_time, quant_bandwith))\n",
    "    for _ in range(10):\n",
    "        torch_output = torch.matmul(a, b)\n",
    "    torch.cuda.synchronize()\n",
    "    iters = 512\n",
    "    t1 = time.time()\n",
    "    for _ in range(iters):\n",
    "        torch_output = torch.matmul(a, b)\n",
    "    torch.cuda.synchronize()\n",
    "    t2 = time.time()\n",
    "    torch_time = t2 - t1\n",
    "    torch_tflops = 2 * M * N * K * 1e-12 / (torch_time / iters)\n",
    "    print(\"Torch time cost: {} (tflops {})\".format(t2 - t1, torch_tflops))\n",
    "\n",
    "    assert torch_time / triton_time > thr, f\"Must get at least {thr}x speedup\"\n",
    "    return triton_time, torch_time, quant_time\n",
    "\n",
    "\n",
    "def test_perf_model_layer(bs, seq_len, hidden, inter, tp, thr: float = 0.99):\n",
    "    st1 = 0\n",
    "    st2 = 0\n",
    "    st3 = 0\n",
    "    t1, t2, t3 = test_perf_matmul_int8(bs * seq_len, hidden, hidden * 3 // tp, thr=thr)\n",
    "    test_perf_quantize(bs * seq_len, hidden, thr=thr)\n",
    "    st1 += t1\n",
    "    st2 += t2\n",
    "    st3 += t3\n",
    "    t1, t2, t3 = test_perf_matmul_int8(bs * seq_len, hidden // tp, hidden, thr=thr)\n",
    "    test_perf_quantize(bs * seq_len, hidden // tp, thr=thr)\n",
    "    st1 += t1\n",
    "    st2 += t2\n",
    "    st3 += t3\n",
    "    t1, t2, t3 = test_perf_matmul_int8(bs * seq_len, hidden, inter * 2 // tp, thr=thr)\n",
    "    st1 += t1\n",
    "    st2 += t2\n",
    "    st3 += t3\n",
    "    t1, t2, t3 = test_perf_matmul_int8(bs * seq_len, inter // tp, hidden, thr=thr)\n",
    "    test_perf_quantize(bs * seq_len, inter // tp, thr=thr)\n",
    "    st1 += t1\n",
    "    st2 += t2\n",
    "    st3 += t3\n",
    "    print(\"Triton time {} Torch time {} Quant time {}\".format(st1, st2, st3))\n",
    "    assert st2 / st1 > thr, f\"Must get at least {thr}x speedup\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "p8lfPXCJlmtX"
   },
   "outputs": [],
   "source": [
    "bs = 32\n",
    "hidden = model.config.hidden_size\n",
    "inter  = model.config.intermediate_size\n",
    "prefill_len = 512\n",
    "decode_len = 1\n",
    "tp = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TjCbfQOolmtX",
    "outputId": "e12f1a1e-0586-4ff8-f152-e8eefdabf6e5"
   },
   "outputs": [],
   "source": [
    "test_perf_model_layer(bs, prefill_len, hidden, inter, tp, thr=1.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: в последней строке видим интегральное ускорение по слою на prefill стадии в целых ~x1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SQKwB1pqlmtY",
    "outputId": "5b18ba51-56db-40f6-f202-9536aa7a6340"
   },
   "outputs": [],
   "source": [
    "test_perf_model_layer(bs, decode_len, hidden, inter, tp, thr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: в последней строке видим интегральное ускорение по слою на decode стадии, оно получилось сильно меньше (x1.03), для того чтобы выжать на decode больше уже нужна реализация на CUDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cmp2zvP7lmtb"
   },
   "source": [
    "Benchmarks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "6JLgqBVblmtb"
   },
   "outputs": [],
   "source": [
    "@triton.autotune(\n",
    "    configs=get_autotune_config(),\n",
    "    key=['M', 'N', 'K'],\n",
    "    reset_to_zero=['c_ptr']\n",
    ")\n",
    "@triton.jit\n",
    "def matmul_kernel(\n",
    "    # Pointers to matrices\n",
    "    a_ptr, b_ptr, c_ptr,\n",
    "    # Matrix dimensions\n",
    "    M, N, K,\n",
    "    # The stride variables represent how much to increase the ptr by when moving by 1\n",
    "    # element in a particular dimension. E.g. `stride_am` is how much to increase `a_ptr`\n",
    "    # by to get the element one row down (A has M rows).\n",
    "    stride_am, stride_ak,\n",
    "    stride_bk, stride_bn,\n",
    "    stride_cm, stride_cn,\n",
    "    # Meta-parameters\n",
    "    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,\n",
    "    GROUP_SIZE_M: tl.constexpr, SPLIT_K: tl.constexpr,\n",
    "):\n",
    "    \"\"\"Kernel for computing the matmul C = A x B.\n",
    "    A has shape (M, K), B has shape (K, N) and C has shape (M, N)\n",
    "    \"\"\"\n",
    "    # -----------------------------------------------------------\n",
    "    # Map program ids `pid` to the block of C it should compute.\n",
    "    # This is done in a grouped ordering to promote L2 data reuse.\n",
    "    # See above `L2 Cache Optimizations` section for details.\n",
    "    pid = tl.program_id(axis=0)\n",
    "    pid_sp_k = tl.program_id(axis=1)\n",
    "    num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)\n",
    "    num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)\n",
    "    num_pid_in_group = GROUP_SIZE_M * num_pid_n\n",
    "    group_id = pid // num_pid_in_group\n",
    "    first_pid_m = group_id * GROUP_SIZE_M\n",
    "    group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)\n",
    "    pid_m = first_pid_m + (pid % group_size_m)\n",
    "    pid_n = (pid % num_pid_in_group) // group_size_m\n",
    "\n",
    "    # ----------------------------------------------------------\n",
    "    # Create pointers for the first blocks of A and B.\n",
    "    # We will advance this pointer as we move in the K direction\n",
    "    # and accumulate\n",
    "    # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers\n",
    "    # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers\n",
    "    # See above `Pointer Arithmetics` section for details\n",
    "    offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M\n",
    "    offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N\n",
    "    offs_k = pid_sp_k * BLOCK_SIZE_K + tl.arange(0, BLOCK_SIZE_K)\n",
    "    a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)\n",
    "    b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)\n",
    "    # -----------------------------------------------------------\n",
    "    # Iterate to compute a block of the C matrix.\n",
    "    # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block\n",
    "    # of fp32 values for higher accuracy.\n",
    "    # `accumulator` will be converted back to fp16 after the loop.\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)\n",
    "    for k in range(0, tl.cdiv(K, BLOCK_SIZE_K * SPLIT_K)):\n",
    "        # Load the next block of A and B, generate a mask by checking the K dimension.\n",
    "        # If it is out of bounds, set it to 0.\n",
    "        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K * SPLIT_K, other=0.0)\n",
    "        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K * SPLIT_K, other=0.0)\n",
    "        # We accumulate along the K dimension.\n",
    "        accumulator += tl.dot(a, b, allow_tf32=False)\n",
    "        # Advance the ptrs to the next K block.\n",
    "        a_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_ak\n",
    "        b_ptrs += BLOCK_SIZE_K * SPLIT_K * stride_bk\n",
    "    # You can fuse arbitrary activation functions here\n",
    "    # while the accumulator is still in FP32!\n",
    "    c = accumulator.to(c_ptr.dtype.element_ty)\n",
    "    # -----------------------------------------------------------\n",
    "    # Write back the block of the output matrix C with masks.\n",
    "    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)\n",
    "    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)\n",
    "    c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]\n",
    "    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)\n",
    "    if SPLIT_K == 1:\n",
    "        tl.store(c_ptrs, c, mask=c_mask)\n",
    "    else:\n",
    "        tl.atomic_add(c_ptrs, c, mask=c_mask)\n",
    "\n",
    "\n",
    "seqlen = inps[0].shape[1]\n",
    "hidden_size = inps[0].shape[2]\n",
    "seqlen, hidden_size\n",
    "\n",
    "\n",
    "def matmul(a, b):\n",
    "    # Check constraints.\n",
    "    assert a.shape[1] == b.shape[0], \"Incompatible dimensions\"\n",
    "    M, K = a.shape\n",
    "    K, N = b.shape\n",
    "    # Allocates output.\n",
    "    c = torch.zeros((M, N), device=a.device, dtype=torch.float16)\n",
    "    grid = lambda META: (\n",
    "        triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']),\n",
    "        META['SPLIT_K'],\n",
    "    )\n",
    "    matmul_kernel[grid](\n",
    "        a, b, c,\n",
    "        M, N, K,\n",
    "        a.stride(0), a.stride(1),\n",
    "        b.stride(0), b.stride(1),\n",
    "        c.stride(0), c.stride(1),\n",
    "    )\n",
    "    return c\n",
    "\n",
    "def prepare_a(M: int):\n",
    "    inp = inps[0][0]\n",
    "    if M > seqlen:\n",
    "        n_repeats = M // seqlen + 1\n",
    "        return inp.repeat(n_repeats, 1)[:M]\n",
    "    else:\n",
    "        return inp[:M]\n",
    "\n",
    "def benchmark(M, provider, provider_funcs):\n",
    "    N = weight.shape[0]\n",
    "    K = weight.shape[1]\n",
    "\n",
    "    a = prepare_a(M).to(torch.float16).cuda()\n",
    "    b = weight.T.to(torch.float16)\n",
    "    assert a.shape == (M, K), f\"{a.shape} != {(M, K)}\"\n",
    "    assert b.shape == (K, N), b.shape\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    ms, min_ms, max_ms = triton.testing.do_bench(lambda: provider_funcs[provider](a, b), quantiles=quantiles)\n",
    "    perf = lambda ms: ms # TFlops = 2 * M * N * K * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)\n",
    "\n",
    "@torch.no_grad()\n",
    "def benchmark_quant_gemm(M, provider, provider_funcs):\n",
    "    N = weight.shape[0]\n",
    "    K = weight.shape[1]\n",
    "\n",
    "    fpa = prepare_a(M).to(torch.float16).cuda().contiguous()\n",
    "    fpb = weight.data.T.to(torch.float16).contiguous()\n",
    "    b, b_scale = quantize_int8(fpb, axis=0)\n",
    "\n",
    "    assert fpa.shape == (M, K), f\"{fpa.shape} != {(M, K)}\"\n",
    "    assert b.shape == (K, N), b.shape\n",
    "\n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    ms, min_ms, max_ms = triton.testing.do_bench(lambda: provider_funcs[provider](fpa, fpb, b, b_scale), quantiles=quantiles)\n",
    "    perf = lambda ms: ms # TFlops = 2 * M * N * K * 1e-12 / (ms * 1e-3)\n",
    "    return perf(ms), perf(max_ms), perf(min_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "53fTTwSTlmtc"
   },
   "outputs": [],
   "source": [
    "provider_funcs = {\n",
    "    \"triton int8\": lambda fpa, fpb, b, b_scale: matmul_quantize_int8(fpa, b, b_scale),\n",
    "    \"triton fp16\": lambda fpa, fpb, b, b_scale: matmul(fpa, fpb),\n",
    "    \"cublas fp16\": lambda fpa, fpb, b, b_scale: torch.matmul(fpa, fpb)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "n8BR7sj2lmtc",
    "outputId": "8cff3c7d-0d42-4c31-b796-f46d6fb05b8b"
   },
   "outputs": [],
   "source": [
    "configs = [\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=[\"M\"],  # Argument names to use as an x-axis for the plot\n",
    "        x_vals=[2 ** i for i in range(7)],  # Different possible values for `x_name`\n",
    "        line_arg=\"provider\",  # Argument name whose value corresponds to a different line in the plot\n",
    "        # Possible values for `line_arg`\n",
    "        line_vals=[\"cublas fp16\", \"triton int8\", \"triton fp16\"],  # Label name for the lines\n",
    "        line_names=[\"cuBLAS FP16\", \"Triton INT8\", \"Triton FP16\"],  # Line styles\n",
    "        styles=[(\"green\", \"-\"), (\"blue\", \"-\"), (\"red\", \"-\")],\n",
    "        ylabel=\"ms\",  # Label name for the y-axis\n",
    "        plot_name=\"GeMV-performance-\" + (\"FP16 vs INT8\"),  # Name for the plot, used also as a file name for saving the plot.\n",
    "        args={\"provider_funcs\": provider_funcs},\n",
    "    ),\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=[\"M\"],  # Argument names to use as an x-axis for the plot\n",
    "        x_vals=[1024 * i for i in range(4, 21)],  # Different possible values for `x_name`\n",
    "        line_arg=\"provider\",  # Argument name whose value corresponds to a different line in the plot\n",
    "        # Possible values for `line_arg`\n",
    "        line_vals=[\"cublas fp16\", \"triton int8\", \"triton fp16\"],  # Label name for the lines\n",
    "        line_names=[\"cuBLAS FP16\", \"Triton INT8\", \"Triton FP16\"],  # Line styles\n",
    "        styles=[(\"green\", \"-\"), (\"blue\", \"-\"), (\"red\", \"-\")],\n",
    "        ylabel=\"ms\",  # Label name for the y-axis\n",
    "        plot_name=\"GeMM-performance-\" + (\"FP16 vs INT8\"),  # Name for the plot, used also as a file name for saving the plot.\n",
    "        args={\"provider_funcs\": provider_funcs},\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "triton.testing.perf_report(configs)(benchmark_quant_gemm).run(show_plots=True, print_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "FV3wWlMOlmtc"
   },
   "outputs": [],
   "source": [
    "provider_funcs = {\n",
    "    \"torch\": lambda a, b: quantize_int8(a),\n",
    "    \"triton\": lambda a, b: quantize_int8_perrow(a)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kPWhpciPlmtc",
    "outputId": "da337105-ec56-4c42-b3d9-e70207fb7513"
   },
   "outputs": [],
   "source": [
    "configs = [\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=[\"M\"],  # Argument names to use as an x-axis for the plot\n",
    "        x_vals=[2 ** i for i in range(7)],  # Different possible values for `x_name`\n",
    "        line_arg=\"provider\",  # Argument name whose value corresponds to a different line in the plot\n",
    "        # Possible values for `line_arg`\n",
    "        line_vals=[\"torch\", \"triton\"],  # Label name for the lines\n",
    "        line_names=[\"Torch\", \"Triton\"],  # Line styles\n",
    "        styles=[(\"green\", \"-\"), (\"blue\", \"-\")],\n",
    "        ylabel=\"ms\",  # Label name for the y-axis\n",
    "        plot_name=\"Quantize-performance-\" + (\"decoding\"),  # Name for the plot, used also as a file name for saving the plot.\n",
    "        args={\"provider_funcs\": provider_funcs},\n",
    "    ),\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=[\"M\"],  # Argument names to use as an x-axis for the plot\n",
    "        x_vals=[256 * i for i in range(4, 21)],  # Different possible values for `x_name`\n",
    "        line_arg=\"provider\",  # Argument name whose value corresponds to a different line in the plot\n",
    "        # Possible values for `line_arg`\n",
    "        line_vals=[\"torch\", \"triton\"],  # Label name for the lines\n",
    "        line_names=[\"Torch\", \"Triton\"],  # Line styles\n",
    "        styles=[(\"green\", \"-\"), (\"blue\", \"-\")],\n",
    "        ylabel=\"ms\",  # Label name for the y-axis\n",
    "        plot_name=\"Quantize-performance-\" + (\"prefill\"),  # Name for the plot, used also as a file name for saving the plot.\n",
    "        args={\"provider_funcs\": provider_funcs},\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "triton.testing.perf_report(configs)(benchmark).run(show_plots=True, print_data=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "2R9vd2NOlmtc"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
